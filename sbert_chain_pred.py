# -*- coding: utf-8 -*-
"""SBERT_Chain_pred.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1olnI4JZ8dWQ-GAfL5JalfiWb6GajLpsC

## Import libraries
"""

import torch
import torch.functional as F
import torch.nn as nn
import torch.optim as optim
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from keras.preprocessing.sequence import pad_sequences
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import unicodedata
import re
import nltk
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel

data = pd.read_csv('data/all_train.csv')


# k = np.load('embeddings.npy',allow_pickle=True)
# print('lllllen',len(data),len(k))
# Converts the unicode file to ascii
def unicode_to_ascii(s):
    """
    Normalizes latin chars with accent to their canonical decomposition
    """
    return ''.join(c for c in unicodedata.normalize('NFD', s)
                   if unicodedata.category(c) != 'Mn')


def preprocess_sentence(w):
    w = unicode_to_ascii(w.lower().strip())
    # creating a space between a word and the punctuation following it
    # eg: "he is a boy." => "he is a boy ." 
    w = re.sub(r"([?.!,¿])", r" \1 ", w)
    w = re.sub(r'[" "]+', " ", w)
    # replacing everything with space except (a-z, A-Z, ".", "?", "!", ",")
    w = re.sub(r"[^a-zA-Z0-9_?.!,¿]+", " ", w)
    w = w.rstrip().strip()
    # adding a start and an end token to the sentence
    # so that the model know when to start and stop predicting.
    w = '<start> ' + w + ' <end>'
    return w


# Now we do the preprocessing using pandas and lambdas
data["QA"] = data.QA.apply(lambda w: preprocess_sentence(w))
data["TAG"] = data.TAG.apply(lambda w: preprocess_sentence(w))
data.sample(10)

### Encode sentences 
from sentence_transformers import SentenceTransformer

sbert = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# print('start encoding')
# data["QA"] = data.QA.apply(lambda w: sbert.encode(w))
# print('start saving')
# np.save('embeddings.npy',data.QA.values)
# print('end')
data["QA"] = np.load('embeddings.npy', allow_pickle=True)
print('loaded')
data.QA.iloc[0].shape

"""#### Building Vocabulary Index

"""


class LanguageIndex():
    def __init__(self, lang):
        """ lang are the list of phrases from each language"""
        self.lang = lang
        self.word2idx = {}
        self.idx2word = {}
        self.vocab = set()
        self.create_index()

    def create_index(self):
        for phrase in self.lang:
            self.vocab.update(phrase.split(' '))
        self.vocab = sorted(self.vocab)
        self.word2idx['<pad>'] = 0
        for index, word in enumerate(self.vocab):
            self.word2idx[word] = index + 1  # +1 because of pad token
        for word, index in self.word2idx.items():
            self.idx2word[index] = word

        ### Encode only targets


targ_lang = LanguageIndex(data["TAG"].values.tolist())
target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')] for eng in data["TAG"].values.tolist()]

targ_lang.word2idx.keys()

target_tensor[:10]


def max_length(tensor):
    return max(len(t) for t in tensor)


max_length_tar = max_length(target_tensor)
print('maaax',max_length_tar)

target_tensor = pad_sequences(target_tensor, max_length_tar)
print(target_tensor)

input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(data.QA, target_tensor,
                                                                                                shuffle=True,
                                                                                                test_size=0.2)
len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)

"""## Load data into DataLoader for Batching
This is just preparing the dataset so that it can be efficiently fed into the model through batches.
"""

from torch.utils.data import Dataset, DataLoader


class MyData(Dataset):
    def __init__(self, X, y):
        self.data = X
        self.target = y
        self.length = [np.sum(1 - np.equal(x, 0)) for x in X]

    def __getitem__(self, index):
        x = self.data[index]
        y = self.target[index]
        x_len = self.length[index]
        return x, y, x_len

    def __len__(self):
        return len(self.data)


"""## Parameters
Let's define the hyperparameters and other things we need for training our NMT model.
"""

BATCH_SIZE = 64
embedding_dim = 384
units = 1024

# vocab_inp_size = len(.word2idx)
vocab_tar_size = len(targ_lang.word2idx)

train_dataset = MyData(data.QA.values, target_tensor)
# val_dataset = MyData(input_tensor_val, target_tensor_val)

dataset = DataLoader(train_dataset, batch_size=BATCH_SIZE,
                     drop_last=True,
                     shuffle=True)


# val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE,
#                     drop_last=True,
#                     shuffle=True)

class Encoder(nn.Module):
    def __init__(self, embedding_dim, enc_units, batch_sz):
        super(Encoder, self).__init__()
        self.batch_sz = batch_sz
        self.enc_units = enc_units
        self.embedding_dim = embedding_dim
        self.gru = nn.GRU(self.embedding_dim, self.enc_units)
        self.lin = nn.Linear(embedding_dim, enc_units)

    def forward(self, x, device):
        # x = self.embedding(x)
        x = x.view(-1, 384)
        return self.lin(x)

    def initialize_hidden_state(self, device):
        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)


class Decoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):
        super(Decoder, self).__init__()
        self.batch_sz = batch_sz
        self.dec_units = dec_units
        self.enc_units = enc_units
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)
        self.gru = nn.GRU(self.embedding_dim + self.enc_units,
                          self.dec_units,
                          batch_first=True)
        self.fc = nn.Linear(self.enc_units, self.vocab_size)

        self.W1 = nn.Linear(self.enc_units, self.dec_units)
        self.W2 = nn.Linear(self.enc_units, self.dec_units)
        self.V = nn.Linear(self.enc_units, 1)

    def forward(self, x, context_vector):
        # print('coooontext ',context_vector.shape)
        x = self.embedding(x)
        x = torch.cat((context_vector.unsqueeze(1), x), -1)
        output, state = self.gru(x)
        output = output.view(-1, output.size(2))
        x = self.fc(output)
        return x, state

    def initialize_hidden_state(self):
        return torch.zeros((1, self.batch_sz, self.dec_units))


criterion = nn.CrossEntropyLoss()


def loss_function(real, pred):
    mask = real.ge(1).type(torch.cuda.FloatTensor)
    loss_ = criterion(pred, real) * mask
    return torch.mean(loss_)


# Device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

encoder = Encoder(embedding_dim, units, BATCH_SIZE)
decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)

encoder.to(device)
decoder.to(device)
optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),
                       lr=0.001)


def sort_batch(X, y, lengths):
    lengths, indx = lengths.sort(dim=0, descending=True)
    X = X[indx]
    y = y[indx]
    return X.transpose(0, 1), y, lengths  # transpose (batch x seq) to (seq x batch)


EPOCHS = 4
encoder.batch_sz = 64
encoder.initialize_hidden_state(device)
decoder.batch_sz = 64
decoder.initialize_hidden_state()

for epoch in range(EPOCHS):
    encoder.train()
    decoder.train()
    total_loss = 0

    for (batch, (inp, targ, inp_len)) in enumerate(dataset):
        loss = 0
        # print(len(inp), inp.shape, inp[0].shape, len(targ))
        # xs, ys, lens = sort_batch(inp, targ, inp_len)
        # x_batch = inp.detach().numpy()
        enc_output = encoder(inp.to(device), device)
        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)
        # dec_input = torch.tensor([sbert.encode('<start>')]* BATCH_SIZE).to(torch.long)
        for t in range(1, targ.size(1)):
            predictions, dec_hidden = decoder(dec_input.to(device),
                                              enc_output.to(device))

            loss += loss_function(targ[:, t].long().to(device), predictions.to(device))
            dec_input = targ[:, t].unsqueeze(1)

        batch_loss = (loss / int(targ.size(1)))
        total_loss += batch_loss
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,
                                                         batch,
                                                         batch_loss.detach().item()))


def translate_sentence(encoder, decoder, sentence, max_length=120):
    encoder.eval()
    decoder.eval()
    total_loss = 0
    sentence = sentence.view(1, -1)
    with torch.no_grad():
        enc_output = encoder(sentence.to(device), device)
        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * 1)
        out_sentence = []
        next_word = '<start>'
        while next_word != '<end>':
            predictions, dec_hidden = decoder(dec_input.to(device),
                                              enc_output.to(device))
            dec_input = predictions.argmax(dim=1).unsqueeze(1)
            next_word = targ_lang.idx2word[predictions.squeeze().argmax().item()]
            out_sentence.append(next_word)
            if len(out_sentence) > max_length:
                break

    return out_sentence


encoder.batch_sz = 1
encoder.initialize_hidden_state(device)
decoder.batch_sz = 1
decoder.initialize_hidden_state()

test_sentence = " which films have the same screenwriter of a tree grows in brooklyn"
test_sentence = [sbert.encode(test_sentence)]
# test_sentence = pad_sequences(test_sentence, max_length_inp)
ret = translate_sentence(encoder, decoder, torch.tensor(test_sentence), max_length=max_length_tar)

print(ret)

def predict_sentences(sentences):

    def predict_sentence(test_sentence,k):
        print("here",k)
        test_sentence = preprocess_sentence(test_sentence)
        test_sentence = np.array([sbert.encode(test_sentence)])
        return translate_sentence(encoder, decoder, torch.tensor(test_sentence), max_length=max_length_tar)

    return [predict_sentence(sentence,k) for k,sentence in enumerate(sentences)]


data_test = pd.read_csv('data/all_test.csv').dropna().sample(12000)

print('bda', len(data_test))
tags_pred = predict_sentences(data_test['QA'].values)
print('test', len(data_test))
tags_true = [preprocess_sentence(sentence).split(' ') for sentence in data_test['TAG']]
print('kemel')
tags_original = tags_pred.copy()
true_tags_original = tags_true.copy()


def clean_tags(tags):
    while len(tags) > 0 and tags[0].startswith("<"):
        tags.pop(0)
    if '<end>' in tags:
        return tags[:tags.index('<end>')]
    return [tag for tag in tags if not tag.startswith("<")]


new_tags_pred = [clean_tags(tags) for tags in tags_original]
new_tags_true = [clean_tags(tags) for tags in true_tags_original]

tags_true_processed = np.array([' '.join(words) for words in new_tags_pred])
tags_pred_processed = np.array([' '.join(words) for words in new_tags_true])
print(tags_pred_processed.shape, tags_true_processed.shape)

results = pd.DataFrame(np.array([data_test.QA.values, tags_true_processed, tags_pred_processed]).transpose(),
                       columns=['QA', 'Original', 'Predicted'])
results.to_csv('results_sbert.csv')

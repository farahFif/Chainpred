{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chain_pred.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z579-ISl9Zj6"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrdZZInZP-M5",
        "outputId": "d4341f12-9128-422c-f73b-fc75ba9d1f01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 31.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 39.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=2dd4c950ac1219e455476329141aebaf8bfd119ae497911a77137abd6a2c5ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT20LFmb3jSW"
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences \n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import nltk\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModel\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "913VSLih4lY3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "c4c8b4cd-eb3c-40c4-cebc-4660ee2eabbd"
      },
      "source": [
        "data = pd.read_csv('all_train.csv')\n",
        "data.head(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                           QA  \\\n",
              "0           0        what movies are about [ginger rogers]   \n",
              "1           1     which movies can be described by [moore]   \n",
              "2           2  what films can be described by [occupation]   \n",
              "3           3         which films are about [jacques tati]   \n",
              "4           4         what movies are about [donnie darko]   \n",
              "\n",
              "                                            ANS           TAG  \n",
              "0  Top Hat|Kitty Foyle|The Barkleys of Broadway  has_tags_inv  \n",
              "1               Fahrenheit 9/11|Far from Heaven  has_tags_inv  \n",
              "2      Red Dawn|The Teahouse of the August Moon  has_tags_inv  \n",
              "3                     Mon Oncle|Playtime|Trafic  has_tags_inv  \n",
              "4                                      S. Darko  has_tags_inv  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-911a627c-8bfd-4e31-a385-727c4cf29e0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>QA</th>\n",
              "      <th>ANS</th>\n",
              "      <th>TAG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>what movies are about [ginger rogers]</td>\n",
              "      <td>Top Hat|Kitty Foyle|The Barkleys of Broadway</td>\n",
              "      <td>has_tags_inv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>which movies can be described by [moore]</td>\n",
              "      <td>Fahrenheit 9/11|Far from Heaven</td>\n",
              "      <td>has_tags_inv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>what films can be described by [occupation]</td>\n",
              "      <td>Red Dawn|The Teahouse of the August Moon</td>\n",
              "      <td>has_tags_inv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>which films are about [jacques tati]</td>\n",
              "      <td>Mon Oncle|Playtime|Trafic</td>\n",
              "      <td>has_tags_inv</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>what movies are about [donnie darko]</td>\n",
              "      <td>S. Darko</td>\n",
              "      <td>has_tags_inv</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-911a627c-8bfd-4e31-a385-727c4cf29e0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-911a627c-8bfd-4e31-a385-727c4cf29e0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-911a627c-8bfd-4e31-a385-727c4cf29e0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCUSf31E4m6t"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Normalizes latin chars with accent to their canonical decomposition\n",
        "    \"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z0-9_?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFLV4RCR4pXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "69f582a1-23e6-44fd-9c43-bc352ec52358"
      },
      "source": [
        "# Now we do the preprocessing using pandas and lambdas\n",
        "data[\"QA\"] = data.QA.apply(lambda w: preprocess_sentence(w))\n",
        "data[\"TAG\"] = data.TAG.apply(lambda w: preprocess_sentence(w))\n",
        "data.sample(10)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0                                                 QA  \\\n",
              "285553       70467  <start> the movies that share actors with the ...   \n",
              "144288       48182  <start> what genres do the movies written by a...   \n",
              "223463        8377  <start> who are the actors in the movies direc...   \n",
              "243700       28614  <start> when did the movies directed by the th...   \n",
              "151545       55439  <start> which person wrote the movies starred ...   \n",
              "310938       95852  <start> what were the release years of the mov...   \n",
              "251138       36052  <start> what were the release years of the fil...   \n",
              "319499      104413  <start> what were the release years of the mov...   \n",
              "269315       54229  <start> what types are the films directed by t...   \n",
              "16567        16567  <start> which words describe anchors aweigh <end>   \n",
              "\n",
              "                                                      ANS  \\\n",
              "285553  Victor Schertzinger|Joshua Logan|Raoul Walsh|S...   \n",
              "144288                                              Drama   \n",
              "223463  Simon Yam|Andy Lau|Melissa George|Nick Cheung|...   \n",
              "243700                                1957|1951|1997|1952   \n",
              "151545                                          Jeff Pope   \n",
              "310938                                     2002|2014|2000   \n",
              "251138                                     1965|1972|1963   \n",
              "319499                                               2000   \n",
              "269315             Action|Drama|Horror|Comedy|Documentary   \n",
              "16567         bd-r|gene kelly|frank sinatra|george sidney   \n",
              "\n",
              "                                                      TAG  \n",
              "285553  <start> starred_actors starred_actors_inv dire...  \n",
              "144288             <start> written_by_inv has_genre <end>  \n",
              "223463  <start> directed_by directed_by_inv starred_ac...  \n",
              "243700  <start> directed_by directed_by_inv release_ye...  \n",
              "151545        <start> starred_actors_inv written_by <end>  \n",
              "310938  <start> written_by written_by_inv release_year...  \n",
              "251138  <start> directed_by directed_by_inv release_ye...  \n",
              "319499  <start> written_by written_by_inv release_year...  \n",
              "269315  <start> directed_by directed_by_inv has_genre ...  \n",
              "16567                              <start> has_tags <end>  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5b0fe42-435e-498e-b020-a7bb0ba31119\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>QA</th>\n",
              "      <th>ANS</th>\n",
              "      <th>TAG</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>285553</th>\n",
              "      <td>70467</td>\n",
              "      <td>&lt;start&gt; the movies that share actors with the ...</td>\n",
              "      <td>Victor Schertzinger|Joshua Logan|Raoul Walsh|S...</td>\n",
              "      <td>&lt;start&gt; starred_actors starred_actors_inv dire...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144288</th>\n",
              "      <td>48182</td>\n",
              "      <td>&lt;start&gt; what genres do the movies written by a...</td>\n",
              "      <td>Drama</td>\n",
              "      <td>&lt;start&gt; written_by_inv has_genre &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223463</th>\n",
              "      <td>8377</td>\n",
              "      <td>&lt;start&gt; who are the actors in the movies direc...</td>\n",
              "      <td>Simon Yam|Andy Lau|Melissa George|Nick Cheung|...</td>\n",
              "      <td>&lt;start&gt; directed_by directed_by_inv starred_ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243700</th>\n",
              "      <td>28614</td>\n",
              "      <td>&lt;start&gt; when did the movies directed by the th...</td>\n",
              "      <td>1957|1951|1997|1952</td>\n",
              "      <td>&lt;start&gt; directed_by directed_by_inv release_ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151545</th>\n",
              "      <td>55439</td>\n",
              "      <td>&lt;start&gt; which person wrote the movies starred ...</td>\n",
              "      <td>Jeff Pope</td>\n",
              "      <td>&lt;start&gt; starred_actors_inv written_by &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310938</th>\n",
              "      <td>95852</td>\n",
              "      <td>&lt;start&gt; what were the release years of the mov...</td>\n",
              "      <td>2002|2014|2000</td>\n",
              "      <td>&lt;start&gt; written_by written_by_inv release_year...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251138</th>\n",
              "      <td>36052</td>\n",
              "      <td>&lt;start&gt; what were the release years of the fil...</td>\n",
              "      <td>1965|1972|1963</td>\n",
              "      <td>&lt;start&gt; directed_by directed_by_inv release_ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319499</th>\n",
              "      <td>104413</td>\n",
              "      <td>&lt;start&gt; what were the release years of the mov...</td>\n",
              "      <td>2000</td>\n",
              "      <td>&lt;start&gt; written_by written_by_inv release_year...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269315</th>\n",
              "      <td>54229</td>\n",
              "      <td>&lt;start&gt; what types are the films directed by t...</td>\n",
              "      <td>Action|Drama|Horror|Comedy|Documentary</td>\n",
              "      <td>&lt;start&gt; directed_by directed_by_inv has_genre ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16567</th>\n",
              "      <td>16567</td>\n",
              "      <td>&lt;start&gt; which words describe anchors aweigh &lt;end&gt;</td>\n",
              "      <td>bd-r|gene kelly|frank sinatra|george sidney</td>\n",
              "      <td>&lt;start&gt; has_tags &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5b0fe42-435e-498e-b020-a7bb0ba31119')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a5b0fe42-435e-498e-b020-a7bb0ba31119 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a5b0fe42-435e-498e-b020-a7bb0ba31119');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqM7ZncM8V9B"
      },
      "source": [
        "#### Building Vocabulary Index\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rXA7-N34sok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc3f2b5-7ff2-4b65-a0a6-adfff31fd54b"
      },
      "source": [
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word      \n",
        "\n",
        "\n",
        "inp_lang = LanguageIndex(data[\"QA\"].values.tolist())\n",
        "targ_lang = LanguageIndex(data[\"TAG\"].values.tolist())\n",
        "input_tensor = [[inp_lang.word2idx[s] for s in es.split(' ')]  for es in data[\"QA\"].values.tolist()]\n",
        "target_tensor = [[targ_lang.word2idx[s] for s in eng.split(' ')]  for eng in data[\"TAG\"].values.tolist()]\n",
        "input_tensor[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[191, 25724, 16627, 1175, 264, 9402, 20321, 190],\n",
              " [191, 25736, 16627, 3840, 2087, 6271, 3677, 16481, 190],\n",
              " [191, 25724, 8277, 3840, 2087, 6271, 3677, 17517, 190],\n",
              " [191, 25736, 8277, 1175, 264, 11908, 23648, 190],\n",
              " [191, 25724, 16627, 1175, 264, 6706, 5854, 190],\n",
              " [191, 25724, 16627, 3840, 2087, 6271, 25971, 2278, 22922, 190],\n",
              " [191, 25724, 8277, 1175, 264, 6476, 190],\n",
              " [191, 25724, 16627, 3840, 2087, 6271, 25971, 4588, 17350, 190],\n",
              " [191, 25724, 16627, 1175, 264, 9213, 17962, 190],\n",
              " [191, 25724, 16627, 1175, 264, 20285, 14629, 190]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fesymsn34v7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5c9d33-e0b1-4c7b-c50a-540113d4e1ed"
      },
      "source": [
        "target_tensor[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1],\n",
              " [2, 9, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cwX-0rt4zmN"
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66dJPqzV44jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331f5bc5-f8ea-47e4-aaeb-825ea28a4b6e"
      },
      "source": [
        "\n",
        "input_tensor = pad_sequences(input_tensor, max_length_inp)\n",
        "target_tensor = pad_sequences(target_tensor, max_length_tar)\n",
        "len(target_tensor)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "329282"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvatfCWS46T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d6bff9-8542-46b9-d5ca-1e0e125ff6bd"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor,shuffle=True, test_size=0.2)\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(263425, 263425, 65857, 65857)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNFO3obpOsoB"
      },
      "source": [
        "## Load data into DataLoader for Batching\n",
        "This is just preparing the dataset so that it can be efficiently fed into the model through batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QRQKwxf479Q"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDSxA4OM5Qlp"
      },
      "source": [
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2WukeVF8NVn"
      },
      "source": [
        "## Parameters\n",
        "Let's define the hyperparameters and other things we need for training our NMT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3Be7lOZ5R-d"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blYXo7pv5TOu"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, device):\n",
        "        x = self.embedding(x)\n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        output, self.hidden = self.gru(x, self.hidden) \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4djvgil5bMQ"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        x = self.embedding(x)\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        output, state = self.gru(x)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QclyWIop5dRG"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "def loss_function(real, pred):\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjMMYJv85hVT"
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpQU6MhEUfFL"
      },
      "source": [
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]    \n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN8G-3YY8ADm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99d430a-d6d8-4cc7-c47b-0645d9bed2b9"
      },
      "source": [
        "EPOCHS = 1\n",
        "def eval2(encoder, decoder, sentence, max_length=120):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0\n",
        "    \n",
        "    sentence = torch.unsqueeze(sentence, dim=1)\n",
        "    with torch.no_grad():\n",
        "        print(sentence.size())\n",
        "        enc_output, enc_hidden = encoder(sentence.to(device), [sentence.size(0)], device)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * 1)\n",
        "        out_sentence = []\n",
        "        for t in range(1, sentence.size(0)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                        dec_hidden.to(device), \n",
        "                                        enc_output.to(device))\n",
        "            dec_input = predictions.argmax(dim=1).unsqueeze(1)\n",
        "            # print(dec_input)\n",
        "            out_sentence.append(targ_lang.idx2word[predictions.squeeze().argmax().item()])\n",
        "            # print(out_sentence)\n",
        "            \n",
        "            # print(predictions.size())\n",
        "    return out_sentence\n",
        "\n",
        "\n",
        "encoder.batch_sz = 64\n",
        "encoder.initialize_hidden_state(device)\n",
        "decoder.batch_sz = 64\n",
        "decoder.initialize_hidden_state()\n",
        "\n",
        "for epoch in range(EPOCHS):    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), device)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            \n",
        "            loss += loss_function(ys[:, t].long().to(device), predictions.to(device))\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "\n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "       \n",
        "        \n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0728\n",
            "Epoch 1 Batch 100 Loss 0.0334\n",
            "Epoch 1 Batch 200 Loss 0.0013\n",
            "Epoch 1 Batch 300 Loss 0.0007\n",
            "Epoch 1 Batch 400 Loss 0.0003\n",
            "Epoch 1 Batch 500 Loss 0.0002\n",
            "Epoch 1 Batch 600 Loss 0.1978\n",
            "Epoch 1 Batch 700 Loss 0.0139\n",
            "Epoch 1 Batch 800 Loss 0.0003\n",
            "Epoch 1 Batch 900 Loss 0.0001\n",
            "Epoch 1 Batch 1000 Loss 0.0001\n",
            "Epoch 1 Batch 1100 Loss 0.0001\n",
            "Epoch 1 Batch 1200 Loss 0.0001\n",
            "Epoch 1 Batch 1300 Loss 0.0000\n",
            "Epoch 1 Batch 1400 Loss 0.0000\n",
            "Epoch 1 Batch 1500 Loss 0.0001\n",
            "Epoch 1 Batch 1600 Loss 0.0000\n",
            "Epoch 1 Batch 1700 Loss 0.0000\n",
            "Epoch 1 Batch 1800 Loss 0.0000\n",
            "Epoch 1 Batch 1900 Loss 0.0000\n",
            "Epoch 1 Batch 2000 Loss 0.0000\n",
            "Epoch 1 Batch 2100 Loss 0.0011\n",
            "Epoch 1 Batch 2200 Loss 0.0001\n",
            "Epoch 1 Batch 2300 Loss 0.0002\n",
            "Epoch 1 Batch 2400 Loss 0.0000\n",
            "Epoch 1 Batch 2500 Loss 0.0000\n",
            "Epoch 1 Batch 2600 Loss 0.0000\n",
            "Epoch 1 Batch 2700 Loss 0.0000\n",
            "Epoch 1 Batch 2800 Loss 0.0074\n",
            "Epoch 1 Batch 2900 Loss 0.0000\n",
            "Epoch 1 Batch 3000 Loss 0.0000\n",
            "Epoch 1 Batch 3100 Loss 0.0000\n",
            "Epoch 1 Batch 3200 Loss 0.0002\n",
            "Epoch 1 Batch 3300 Loss 0.0000\n",
            "Epoch 1 Batch 3400 Loss 0.0005\n",
            "Epoch 1 Batch 3500 Loss 0.0003\n",
            "Epoch 1 Batch 3600 Loss 0.0000\n",
            "Epoch 1 Batch 3700 Loss 0.0000\n",
            "Epoch 1 Batch 3800 Loss 0.0000\n",
            "Epoch 1 Batch 3900 Loss 0.0000\n",
            "Epoch 1 Batch 4000 Loss 0.0005\n",
            "Epoch 1 Batch 4100 Loss 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vYBffn-2Hao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4295510-896d-451c-bd63-7f6306fdd045"
      },
      "source": [
        "# def translate_sentence(encoder, decoder, sentence, max_length=120):\n",
        "#     encoder.eval()\n",
        "#     decoder.eval()\n",
        "    # total_loss = 0\n",
        "    # sentence = sentence.transpose(0,1) \n",
        "    # with torch.no_grad():\n",
        "    #     enc_output, enc_hidden = encoder(sentence.to(device),device)\n",
        "    #     dec_hidden = enc_hidden\n",
        "    #     dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * 1)\n",
        "    #     out_sentence = []\n",
        "    #     for t in range(1, sentence.size(0)):\n",
        "    #         predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "    #                                     dec_hidden.to(device), \n",
        "    #                                     enc_output.to(device))\n",
        "    #         dec_input = predictions.argmax(dim=1).unsqueeze(1)\n",
        "    #         out_sentence.append(targ_lang.idx2word[predictions.squeeze().argmax().item()])\n",
        "\n",
        "    # return out_sentence\n",
        "def translate_sentence(encoder, decoder, sentence, max_length=120):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0\n",
        "    sentence = sentence.transpose(0, 1)\n",
        "    with torch.no_grad():\n",
        "        enc_output, enc_hidden = encoder(sentence.to(device), device)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * 1)\n",
        "        out_sentence = []\n",
        "        for t in range(1, sentence.size(0)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device),\n",
        "                                                 dec_hidden.to(device),\n",
        "                                                 enc_output.to(device))\n",
        "            dec_input = predictions.argmax(dim=1).unsqueeze(1)\n",
        "            next_word = targ_lang.idx2word[predictions.squeeze().argmax().item()]\n",
        "            out_sentence.append(next_word)\n",
        "            if next_word == '<end>':\n",
        "                break\n",
        "            \n",
        "\n",
        "    return out_sentence\n",
        "\n",
        "\n",
        "encoder.batch_sz = 1\n",
        "encoder.initialize_hidden_state(device)\n",
        "decoder.batch_sz = 1\n",
        "decoder.initialize_hidden_state()\n",
        "\n",
        "test_sentence = \"<start> which films have the same screenwriter of a tree grows in brooklyn <end>\"\n",
        "test_sentence = [[inp_lang.word2idx[s] for s in test_sentence.split(' ')]]\n",
        "test_sentence = pad_sequences(test_sentence, max_length_inp)\n",
        "ret = translate_sentence(encoder, decoder, torch.tensor(test_sentence), max_length=max_length_tar)\n",
        "ret"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start>', 'written_by', 'written_by_inv', '<end>']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def predict_sentences(sentences):\n",
        "    def predict_sentence(test_sentence):\n",
        "        test_sentence = preprocess_sentence(test_sentence)\n",
        "        test_sentence = [[inp_lang.word2idx[s] for s in test_sentence.split(' ') if s in inp_lang.word2idx]]\n",
        "        test_sentence = pad_sequences(test_sentence, max_length_inp)\n",
        "        return translate_sentence(encoder, decoder, torch.tensor(test_sentence), max_length=max_length_tar)\n",
        "\n",
        "    return [predict_sentence(sentence) for sentence in sentences]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NV5yiIj7jOp7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_test = pd.read_csv('all_test.csv').dropna().iloc[:2000]\n",
        "\n",
        "tags_pred = predict_sentences(data_test['QA'].values)\n",
        "tags_true = [preprocess_sentence(sentence).split(' ') for sentence in data_test['TAG']]\n",
        "\n"
      ],
      "metadata": {
        "id": "GZTCTiC0hFfP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags_original = tags_pred.copy()\n",
        "true_tags_original = tags_true.copy()\n",
        "def clean_tags(tags):\n",
        "    while len(tags) > 0 and tags[0].startswith(\"<\"):\n",
        "        tags.pop(0)\n",
        "    if '<end>' in tags:\n",
        "        return tags[:tags.index('<end>')]\n",
        "    return [tag for tag in tags if not tag.startswith(\"<\")]\n",
        "new_tags_pred = [clean_tags(tags) for tags in tags_original]\n",
        "new_tags_true = [clean_tags(tags) for tags in true_tags_original]\n",
        "print(new_tags_pred[0], new_tags_true[0])"
      ],
      "metadata": {
        "id": "-MwIZFoou4TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98db3e57-6b23-49d0-eb83-2906a84654e1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['starred_actors_inv'] ['starred_actors_inv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_test.QA.shape, len(tags_true[0]), len(tags_pred[0]))\n",
        "tags_true_processed = np.array([' '.join(words) for words in new_tags_pred])\n",
        "tags_pred_processed = np.array([' '.join(words) for words in new_tags_true])\n",
        "print(tags_pred_processed.shape, tags_true_processed.shape)"
      ],
      "metadata": {
        "id": "hkqsGus0O-UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame(np.array([data_test.QA.values,tags_true_processed,tags_pred_processed]).transpose(),\n",
        "                       columns=['QA','Original','Predicted'])\n",
        "results.to_csv('results.csv')"
      ],
      "metadata": {
        "id": "uy7p59AMq4oM"
      },
      "execution_count": 80,
      "outputs": []
    }
  ]
}